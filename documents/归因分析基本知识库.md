# 归因分析-维度归因

维度归因是归因报告的一项重要功能，可以通过自动化数据归因模块，帮助用户对指标的变化在各个维度上进行拆解，并给出定量的贡献解释。通过维度归因，用户可以深入了解大盘指标的变化原因，并明确各个维度对指标变化的影响程度

首先，归因计算需要有几个必须输入

1. **核心指标**: 选择一个核心指标
2. **待分析指标**：选择待分析的维度指标
3. **聚合方式**：加法，乘法，除法。

以及偏好内容，包括越高越好，越低越好，或者无。其次，在指定好核心指标之后，需要选择贡献率算法。以下是**公式和选取算法**之间的映射关系：

| **贡献率算法** | **公式**                                                     |
| -------------- | ------------------------------------------------------------ |
| 定基法         | sum(); count(); count(distinct); sum()/sum(); sum()/count(distinct) |
| 占比加权法     | sum()/sum(); sum()/count() 分子分母都可加的除法              |


## 贡献率算法

### 定基法

定基法，全称定基替代法，是用于解决变化分解问题的算法。 其核心思想是，假设其他维值保持不变，只有一个维值发生变化，这时给总指标带来的变化，就是这个维值的贡献。
下面展示不同类型指标下，定基法的详细计算方法。 注意，定基法待分析的维度的值一定可通过公式得到核心指标，且所有指标直接参与计算。

#### 1.1.2 算法配置

**（1）适用范围**
假设数据集表为

| **a_int** | **b_int** | **c_int** | **d_string** |
| --------- | --------- | --------- | ------------ |
| 1         | 2         | 3         | “啊啊啊”     |
| 4         | 5         | 6         | “哈哈哈”     |

**（2）直接表达式配置**

| **核心指标**                           | **聚合方式** |
| -------------------------------------- | ------------ |
| a_int                                  | 求和         |
| a_int + b_int                          | 求和         |
| case when a_int > 10 then 1 else 0 end | 计数         |
| d_string                               | 计数不同     |


**（3）加法配置**

| **核心指标**                                             | **聚合方式** |
| -------------------------------------------------------- | ------------ |
| sum(a_int)                                               | 聚合         |
| sum(a_int) + sum(b_int)                                  | 聚合         |
| sum(a_int) + sum(case when a_int > 10 then 1 else 0 end) | 聚合         |
| sum(a_int) + count(b_int)                                | 聚合         |
| sum(a_int) + 100                                         | 聚合         |
| count(distinct a_int) - count(a_int)                     | 聚合         |



**4）除法配置**

| **核心指标**                               | **聚合方式** |
| ------------------------------------------ | ------------ |
| sum(a_int) / sum(b_int)                    | 聚合         |
| sum(a_int) / count(b_int)                  | 聚合         |
| sum(a_int) / count(distinct b_int)         | 聚合         |
| ( sum(a_int) + sum(b_int) ) / count(c_int) | 聚合         |
| count(distinct a_int) / count(a_int)       | 聚合         |
| avg(a_int)                                 | 聚合         |

> avg(a_int) 可以视为 sum(a_int)/count(a_int)



**（5）其他复杂类型配置**

| **核心指标**                                                 | **聚合方式** |
| ------------------------------------------------------------ | ------------ |
| sum(a_int) + ( sum(b_int) / count(c_int) )                   | 聚合         |
| ( * sum(b_int) ) / count(c_int)                   | 聚合         |
| ( sum(a_int) * sum(b_int) ) / ( count(c_int) * count(c_int) ) | 聚合         |
| sum(a_int) / sum(b_int) + sum(a_int) / count(b_int)          | 聚合         |
| avg(a_int) - avg(b_int)                                      | 聚合         |
sum(a_int) 


##### 加法指标的算法说明（原Delta法）
(1) 计算方法
维度项变化值在父层级变化值的占比

$$ 维度i的贡献率 = \frac{维度i当前值 - 维度i参考值}{核心指标值 - 参考指标值} $$

所有维度的贡献率相加应等于1。贡献率有正值和负值

**同向影响因子**：对大盘指标同向变动做出贡献的因子，贡献率为正的因子会判定为同向影响因子。
**反向影响因子**：对大盘指标反向变动做出贡献的因子，贡献率为负的因子会判定为反向影响因子。



##### 除法指标的算法说明

举个例子，当**分子和分母是可加和**的，如：满意量=处理组A的满意量+处理组B的满意量+……+处理组N的满意量，那么审核准确率指标，是一个复杂的除法复合指标。 满意度=满意量/参评量。
**（1）计算方法**
使用今天的维度值指标代替昨天的维度值指标，得到与昨天大盘的差值

$$ \frac{F(m_1) -F_{ij}(m_1) + A_{ij}(m_1)}{F(m_2) - F_{ij}(m_2) + A_{ij}(m+2)} - \frac{F(m_1)}{F(m_2)} $$ 

$$ \frac{F(m_2) F(m_1) + F(m_2)[A_{ij}(m_1) - F_{ij}(m_1)] - (F(m_1)F(m_2) + F(m_1)[A_{ij}(m_2) - F_{ij}(m_2)])}{F(m_2)(F(m_2) + A_{ij}(m_2) - F_{ij}(m_2))}$$

$$ \frac{F(m_2)[A_{ij}(m_1) - F_{ij}(m_1)] - (F(m_1)[A_{ij}(m_2) - F_{ij}(m_2)])}{F_{m_2}(F(m_2) + A_{ij}(m_2) - F_{ij}(m_2))} $$

举例，其中EP值是归因得到的贡献值

- 分子变化情况

| **维度项** | **基础期分子** | **基础期分母** | **基础期指标** | **对比期分子** | **对比期分母** | **对比期指标** | **EP**       |
| ---------- | -------------- | -------------- | -------------- | -------------- | -------------- | -------------- | ------------ |
| A          | 10             | 10             | 1              | 5              | 10             | 0.5            | -0.166666667 |
| B          | 1              | 10             | 0.1            | 10             | 10             | 1              | 0.3          |
| C          | 19             | 10             | 1.9            | 19             | 10             | 1.9            | 0            |
| 总         | 30             | 30             | 1              | 34             | 30             | 1.133333333    | 0.133333333  |

- 分母变化情况

| **维度项** | **基础期分子** | **基础期分母** | **基础期指标** | **对比期分子** | **对比期分母** | **对比期指标** | **EP**       |
| ---------- | -------------- | -------------- | -------------- | -------------- | -------------- | -------------- | ------------ |
| A          | 10             | 10             | 1              | 10             | 50             | 0.2            | -0.571428571 |
| B          | 1              | 10             | 0.1            | 1              | 2              | 0.5            | 0.363636364  |
| C          | 19             | 10             | 1.9            | 19             | 10             | 1.9            | 0            |
| 总         | 30             | 30             | 1              | 30             | 62             | 0.483870968    | -0.516129032 |

- 分子分母都变化情况

| **维度项** | **基础期分子** | **基础期分母** | **基础期指标** | **对比期分子** | **对比期分母** | **对比期指标** | **EP**  |
| ---------- | -------------- | -------------- | -------------- | -------------- | -------------- | -------------- | ------- |
| A          | 10             | 10             | 1              | 5              | 60             | 0.083333333    | -0.6875 |
| B          | 1              | 10             | 0.1            | 10             | 100            | 0.1            | -0.675  |
| C          | 19             | 10             | 1.9            | 19             | 10             | 1.9            | 0       |
| 总         | 30             | 30             | 1              | 34             | 170            | 0.2            | -0.8    |



##### 组合指标的算法说明

组合指标，即指标由多个基础指标组合运算得到。这时需要使用 **差分法** 计算维值对组合指标带来的变化。
设$f$、$g$、$h$ 为基础指标在基期的值 $\Delta{f}$、$\Delta{g}$、$\Delta{h}$ 为某维值给基础指标带来的变化，$a$、$b$、$c$ 为常数，则该维值对组合指标带来的变化，可通过在组合公式上使用差分法得到：

$$ \Delta(\frac{f}{g}) = \frac{f + \Delta{f}}{g + \Delta{g}} - \frac{f}{g} = \frac{g \Delta{f} - f \Delta{g}}{g\cdot (g+\Delta{g})}$$

$$ \Delta (\frac{af + bh}{cg}) = \frac{a(f+\Delta{f}) + b(h+\Delta{h})}{c(g+\Delta{g})} - \frac{af + bh}{cg}$$

这种方法对指标的计算公式没有限制，任意公式均可使用。



##### 乘法指标说明

乘法指标会将其相关的维度取对数值后，转换为加法指标进行分析。



### 机器学习法

一个主要指标（亦称北极星指标）一般和若干个相关指标的波动有联系。Shapley 回归方式，通过计算了每个子指标对于主指标的贡献程度，说明了子指标的波动和主指标的波动相关联及其关联程度。这种方式的一个优点在于，可以支持用户把所有相关指标的贡献加和进而得到主指标的波动程度 （Additive Property），以此增加了可解释性。所以本产品首先对主指标和子指标拟合了一个模型，然后利用 Shapley 的回归值解释每一个子指标的贡献。

本产品将 XGBoost 作为使用的基本模型，把历史数据切割成训练和验证集合。如果训练的模型在测试集上表现良好，则认为**模型预测效果好并且归因结果可靠；**否则本产品会提示用户归因结果不可靠，请考虑提供更多的数据或者增加相关性指标。



#### Shapley值归因解释

SHAP Value 是一种解释机器学习模型预测结果的工具，它能够帮助我们理解每个特征（即输入变量）对最终预测结果的贡献。

假设我们在做一个团队项目，每个人都有自己的任务和贡献，项目的最终成绩就是所有人的努力汇总的结果。SHAP Value 就像是对每个人在项目中贡献的打分，它告诉我们每个人的贡献是正面的还是负面的，以及贡献的大小。在机器学习模型中，模型的预测结果就是“项目的最终成绩”，每个特征就像团队中的成员。SHAP Value 解释了每个特征对预测结果的影响：是增加了预测值还是减少了预测值，贡献的程度又有多大。

##### 关键点
1. **透明度**：SHAP Value 让我们清楚地看到模型是如何得出预测结果的，每个特征具体起了什么作用。
2. **公正性**：SHAP Value 提供了一种公平的方式来分配每个特征对预测结果的贡献，就像在项目中公平地评估每个人的贡献一样。
3. **可视化**：通常，我们会用图表来展示 SHAP Value，这样更容易理解哪些特征对结果有最大影响。

为了帮助不懂数据科学的业务人员理解 SHAP Value，除了比喻和概念的介绍，我们还可以用一些简单的数学公式来说明它的计算方式。以下是一个结合公式和解释的方式。

##### SHAP Value 的数学解释

SHAP（SHapley Additive exPlanations）来源于博弈论中的 Shapley 值，它用于分配合作博弈中的总收益，确保每个参与者得到公平的分配。在机器学习中，我们将这种思想用于分配特征对预测结果的贡献。

###### 公式

SHAP Value 的核心思想是计算每个特征的边际贡献。假设模型有 $ n $ 个特征 $ x_1, x_2, \dots, x_n $，对于每个特征 $i$，它的 SHAP Value 定义为：

$$
\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! \cdot (|N| - |S| - 1)!}{|N|!} \left( f(S \cup \{i\}) - f(S) \right)
$$
这里的符号解释如下：
- $ N $ 是特征的全集，即所有的特征。
- $S$ 是$N$中的一个子集，不包含特征 $i$。
- $|S|$ 表示子集 $S$ 中特征的数量。
-  $f(S)$ 是仅使用子集$S$中的特征时模型的预测值。
- $f(S \cup \{i\})$ 是在加入特征 $i$ 后模型的预测值。

###### 解释

1. **边际贡献**：公式中的 $ f(S \cup \{i\}) - f(S) $ 表示特征 $ i $对模型预测的边际贡献。换句话说，假设我们已经有了子集$ S$的特征，那么增加特征$i $后，模型的预测值发生了多少变化，这就是特征 $i $的边际贡献。

2. **加权平均**：由于我们可能有多个不同的子集 $S $，公式通过计算特征 $i $在所有可能情况下的边际贡献，并按加权平均的方式求出最终的 SHAP Value。权重由$ \frac{|S|! \cdot (|N| - |S| - 1)!}{|N|!} $这个组合数学公式决定，确保每种排列情况都被公平考虑。

3. **整体贡献**：最终的 SHAP Value$ \phi_i $代表了特征 $i $ 在所有可能组合中的平均贡献，反映了该特征对模型预测结果的整体影响。

通过 SHAP Value 的公式，可以看出它在模型解释中的严谨性和公平性。它不仅考虑了特定特征的单独作用，还考虑了它在不同组合下的表现。对于业务人员来说，这个公式表明 SHAP Value 是一种科学且可靠的方法，能够帮助我们更好地理解模型的决策过程。



#### XGBoost 模型解释

XGBoost 是一种非常强大和高效的机器学习算法，它在许多比赛和实际应用中表现出色。为了让不懂数据科学的业务人员理解 XGBoost，我们可以从概念、特点和工作原理三个方面进行解释。

XGBoost 是 "Extreme Gradient Boosting" 的缩写，它是一种基于梯度提升（Gradient Boosting）的决策树集成算法。简单来说，XGBoost 是一个通过结合多个弱学习器（通常是决策树）来构建强大模型的方法。你可以把 XGBoost 想象成一个专家团队，每个专家（即决策树）专注于解决问题的一部分。每个新的专家会学习之前专家没解决好的部分，最后把所有专家的意见综合起来，形成一个非常准确的预测模型。

###### 特点

XGBoost 之所以在实践中广受欢迎，是因为它有以下几个显著的特点：

- **高效**：XGBoost 采用了许多优化技术，如并行计算和增量训练，使它在处理大规模数据时非常高效。
- **准确性**：通过逐步优化的方式，XGBoost 可以极大地减少误差，提高模型的准确性。
- **灵活性**：XGBoost 支持回归、分类、排序等多种任务，同时可以处理缺失值、特征权重等问题。
- **正则化**：XGBoost 在模型训练过程中引入了正则化项，可以防止模型过拟合（即模型在训练数据上表现很好，但在新数据上表现较差）。

##### XGBoost 的工作原理

XGBoost 的核心思想是通过 **Boosting** 的方式来增强模型的表现。Boosting 是一种迭代的算法，它通过逐步修正模型的误差来提高模型的预测能力。XGBoost 使用的 Boosting 方法可以分为以下几个步骤：

###### 1. 初始化模型：
首先，我们构建一个简单的模型，比如一棵决策树，用来预测目标值。

###### 2. 计算残差：
然后，我们计算模型的预测值和真实值之间的差异，这个差异叫做 **残差**。残差代表了模型当前还没能很好解释的部分。

###### 3. 训练新的模型来修正残差：
接着，XGBoost 会构建一个新的决策树，专门用来预测这些残差。这个新模型的目标是修正之前模型的错误。

###### 4. 更新模型：
将新模型的预测结果与之前的模型预测结果结合起来，形成一个更好的预测。

###### 5. 重复迭代：
XGBoost 会重复这个过程，逐步构建多个决策树，每个新树都在改进之前的预测。最终，所有树的预测结果会被加权平均，得到最终的预测值。

###### 6. 正则化和优化：
在训练过程中，XGBoost 通过正则化技术防止模型过于复杂，同时使用了许多优化手段加速训练过程。

总的来说XGBoost 是一种能够通过迭代优化误差、结合多个简单模型来提高预测准确性的机器学习算法。它的高效、灵活和强大的特性使得它在数据科学领域得到了广泛的应用。对于业务人员来说，理解 XGBoost 就像理解一个专家团队的工作方式：每个专家都在解决问题的一部分，最终形成一个高效而准确的解决方案。

XGBoost（Extreme Gradient Boosting）在归因分析中起到了关键作用，特别是在解释机器学习模型的决策过程时。归因分析的目的是确定哪些特征对模型的预测结果贡献最大，而XGBoost是一种基于梯度提升的集成算法，可以通过其内在机制和外部工具实现归因分析。

具体来说，XGBoost在归因分析中的作用体现在以下几个方面：

- 特征重要性分析：XGBoost可以通过计算特征的重要性分数（例如基于信息增益、特征出现次数等）来衡量各个特征对模型预测结果的影响。这种重要性分数可以帮助我们理解哪些特征对模型的决策起到了重要作用。

- **Shapley值**：XGBoost可以与Shapley值（SHAP）结合使用，进行局部归因分析。Shapley值是一种基于博弈论的方法，能够为每个样本计算各个特征的贡献，从而解释模型在特定样本上的预测原因。XGBoost通过SHAP方法，可以细粒度地揭示各个特征在单一预测结果中的作用。

- 局部可解释性：除了全局特征重要性，XGBoost还可以通过与LIME（Local Interpretable Model-agnostic Explanations）等工具结合，实现局部解释。LIME可以生成一个线性模型来逼近XGBoost模型在局部的决策边界，从而解释具体样本的预测结果。

- **模型调优**：在模型调优过程中，归因分析能够帮助识别哪些特征需要更多关注或调整，以提高模型性能。XGBoost提供了强大的特征选择和重要性评估工具，这些工具对于模型的改进和优化非常有价值。
